# ğŸ”„ Multi-Source Data Integration Pipeline

A simple ETL pipeline that integrates data from multiple sources into a unified PostgreSQL warehouse.

## What This Does

Extracts data from:
- ğŸ“„ **CSV files** - Customer master data
- ğŸŒ **REST API** - Real-time product catalog

Combines them into a single PostgreSQL database for unified analytics.

## Business Case

Many companies have data scattered across different systems:
- Customer data in CSV exports from CRM
- Product data in external API (supplier catalog)
- Need unified view for analytics

This pipeline solves that by:
1. **Extracting** data from CSV and API
2. **Validating** data quality
3. **Loading** into single warehouse

## Tech Stack

- **Python 3.8+** - ETL scripts
- **PostgreSQL** - Data warehouse
- **REST API** - External data source (JSONPlaceholder API)
- **pandas** - Data processing

## Quick Start
```bash
# 1. Create database
createdb integration_db

# 2. Create tables
psql -d integration_db -f sql/schema.sql

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure
cp .env.example .env
# Edit .env with your database info

# 5. Run pipeline
python main.py
```

## Project Structure
```
multi-source-integration/
â”œâ”€â”€ README.md
â”œâ”€â”€ SETUP_GUIDE.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ main.py
â”‚
â”œâ”€â”€ config.py              # Configuration
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_csv.py     # CSV extraction
â”‚   â”œâ”€â”€ extract_api.py     # API extraction
â”‚   â”œâ”€â”€ transform.py       # Data transformation
â”‚   â”œâ”€â”€ validate.py        # Data validation
â”‚   â””â”€â”€ load.py           # Load to database
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema.sql        # Database schema
â”‚   â””â”€â”€ queries.sql       # Sample queries
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ customers.csv     # Sample customer data
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_pipeline.py  # Tests
```

## Data Sources

### Source 1: Customer CSV
Local CSV file with customer information:
- customer_id
- name
- email
- signup_date
- country

### Source 2: Product API
JSONPlaceholder API for products:
- product_id
- title
- price
- category

API: `https://jsonplaceholder.typicode.com/users` (demo API)

## What I Learned

- âœ… Multi-source data integration
- âœ… REST API consumption with Python
- âœ… Data validation techniques
- âœ… ETL pipeline design
- âœ… Error handling
- âœ… PostgreSQL integration

## Running the Pipeline
```bash
# Run full pipeline
python main.py

# Test extraction only
python -c "from src.extract_csv import extract_customers; print(extract_customers())"

# Test API extraction
python -c "from src.extract_api import extract_products; print(extract_products())"
```

## Sample Output
```
============================================================
ğŸš€ MULTI-SOURCE INTEGRATION PIPELINE
============================================================

[1/4] EXTRACT - CSV
ğŸ“„ Reading customers from data/customers.csv
âœ… Extracted 50 customers

[2/4] EXTRACT - API
ğŸŒ Fetching products from API
âœ… Extracted 10 products

[3/4] TRANSFORM & VALIDATE
ğŸ” Validating customers data...
âœ… Validation passed
ğŸ” Validating products data...
âœ… Validation passed

[4/4] LOAD
ğŸ“¥ Loading 50 customers to database
âœ… Loaded customers
ğŸ“¥ Loading 10 products to database
âœ… Loaded products

============================================================
âœ… PIPELINE COMPLETED SUCCESSFULLY
============================================================
Duration: 3.2 seconds
```

## Database Schema

Simple normalized schema:

**customers table:**
- customer_id (PK)
- name
- email
- signup_date
- country

**products table:**
- product_id (PK)
- name
- price
- category

## Sample Queries
```sql
-- View all customers
SELECT * FROM customers LIMIT 10;

-- View all products
SELECT * FROM products;

-- Count by country
SELECT country, COUNT(*) 
FROM customers 
GROUP BY country 
ORDER BY COUNT(*) DESC;
```

## Future Enhancements

- [ ] Add more data sources (database, S3)
- [ ] Implement incremental loading
- [ ] Add data quality scoring
- [ ] Create dashboard integration
- [ ] Schedule with cron/Airflow

## Author

**[Your Name]**  
GitHub: [@yourusername](https://github.com/yourusername)  
LinkedIn: [Your Profile](https://linkedin.com/in/yourprofile)

Built to demonstrate multi-source data integration and API consumption skills.

## License

MIT License - see LICENSE file

---

**Last Updated:** January 2026
```

---

#### File: `.gitignore`
```
# Python
__pycache__/
*.py[cod]
*.pyc
venv/
env/
.venv

# Environment
.env

# Data files (keep sample)
data/*.csv
!data/customers.csv

# Logs
*.log
logs/

# IDE
.vscode/
.idea/
*.swp
.DS_Store

# Database
*.db
*.sqlite

# Testing
.pytest_cache/
```

---

#### File: `LICENSE`
```
MIT License

Copyright (c) 2026 [Your Name]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

#### File: `requirements.txt`
```
# Core dependencies
pandas==2.0.3
psycopg2-binary==2.9.9
python-dotenv==1.0.0
requests==2.31.0
```

---

#### File: `.env.example`
```
# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=integration_db
DB_USER=postgres
DB_PASSWORD=your_password

# API Configuration (using public demo API)
API_BASE_URL=https://jsonplaceholder.typicode.com
API_TIMEOUT=30